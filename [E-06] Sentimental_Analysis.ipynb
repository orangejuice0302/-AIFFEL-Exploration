{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f9e847",
   "metadata": {},
   "source": [
    "## ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° í…ìŠ¤íŠ¸ ê°ì„±ë¶„ì„í•˜ê¸° ğŸ’­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2371d",
   "metadata": {},
   "source": [
    "ğŸ’¬ ì˜í™”ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ì´ìš©í•˜ì—¬ ê°ì„± ë¶„ì„í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9c467",
   "metadata": {},
   "source": [
    "### 1. ë°ì´í„° ì¤€ë¹„ì™€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3029016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬      0\n",
       "1   3819312                  í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜      1\n",
       "2  10265843                                  ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤      0\n",
       "3   9045019                      êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •      0\n",
       "4   6483659  ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„°ë¥¼ ì½ì–´ë´…ì‹œë‹¤. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "\n",
    "print(len(train_data), len(test_data))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5fad0",
   "metadata": {},
   "source": [
    "### 2. ë°ì´í„°ë¡œë” êµ¬ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea670909",
   "metadata": {},
   "source": [
    "ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ë°ì´í„°ë¥¼ ì‚´í´ë³´ì.\n",
    "\n",
    "ë¨¼ì €, ì¤‘ë³µë˜ëŠ” ê°’ì´ ìˆëŠ”ì§€ ë³¸ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622f8425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146182"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f355d7",
   "metadata": {},
   "source": [
    "3818ê°œì˜ ì¤‘ë³µ ë°ì´í„°ê°€ ì¡´ì¬í•œë‹¤. -> **ì‚­ì œí•´ì£¼ì–´ì•¼ í•¨!**\n",
    "\n",
    "ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea3e9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 0\n",
      "document : 5\n",
      "label : 0\n"
     ]
    }
   ],
   "source": [
    "for c in train_data.columns:\n",
    "    print('{} : {}'.format(c, len(train_data.loc[pd.isnull(train_data[c]), c].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c65aab",
   "metadata": {},
   "source": [
    "`document` ì—´ì— 5ê°œì˜ `null`ë°ì´í„°ê°€ ìˆë‹¤. -> **ì‚­ì œí•´ì£¼ì–´ì•¼ í•¨!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c545bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Wrong number or type of arguments for overloaded function 'Tagger_parse'.\n  Possible C/C++ prototypes are:\n    MeCab::Tagger::parse(MeCab::Model const &,MeCab::Lattice *)\n    MeCab::Tagger::parse(MeCab::Lattice *) const\n    MeCab::Tagger::parse(char const *)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/914131933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36mmorphs\u001b[0;34m(self, phrase)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;34m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, flatten, join)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/MeCab.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0m__getattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Wrong number or type of arguments for overloaded function 'Tagger_parse'.\n  Possible C/C++ prototypes are:\n    MeCab::Tagger::parse(MeCab::Model const &,MeCab::Lattice *)\n    MeCab::Tagger::parse(MeCab::Lattice *) const\n    MeCab::Tagger::parse(char const *)\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']\n",
    "\n",
    "X_train = []\n",
    "for x in train_data['document']:\n",
    "    tokens = tokenizer.morphs(x)\n",
    "    if tokens not in stopwords:\n",
    "        X_train.append(tokens)\n",
    "    \n",
    "X_test = []\n",
    "for x in test_data['document']:\n",
    "    tokens = tokenizer.morphs(x)\n",
    "    if tokens not in stopwords:\n",
    "        X_train.append(tokens)\n",
    "            \n",
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93a54fcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/232191445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ì˜'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ê°€'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ì´'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ì€'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ë“¤'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ëŠ”'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ì¢€'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ì˜'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ê±'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ê³¼'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ë„'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ë¥¼'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ìœ¼ë¡œ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ì'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ì—'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ì™€'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'í•œ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'í•˜ë‹¤'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# ë°ì´í„° ì¤‘ë³µ ì œê±°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_words' is not defined"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=num_words):\n",
    "    \n",
    "    # ë°ì´í„° ì¤‘ë³µ ì œê±°\n",
    "    train_data.drop_duplicates(['document'])\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì œê±°\n",
    "    train_data = train_data.dropna(how = 'any')\n",
    "    test_data = test_data.dropna(how = 'any')\n",
    "    \n",
    "    ## í† í°í™”(+ stopwords ì œê±°)\n",
    "    X_train = []\n",
    "    for x in train_data['document']:\n",
    "        tokens = tokenizer.morphs(x)\n",
    "        if tokens not in stopwords:\n",
    "            X_train.append(tokens)\n",
    "    \n",
    "    X_test = []\n",
    "    for x in test_data['document']:\n",
    "        tokens = tokenizer.morphs(x)\n",
    "        if tokens not in stopwords:\n",
    "            X_train.append(tokens)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, word_to_index\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff9af69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_to_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/456468877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word_to_index' is not defined"
     ]
    }
   ],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ 1ê°œë¥¼ í™œìš©í•  ë”•ì…”ë„ˆë¦¬ì™€ í•¨ê»˜ ì£¼ë©´, ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë²¡í„°ë¡œ ë³€í™˜í•´ ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "# ë‹¨, ëª¨ë“  ë¬¸ì¥ì€ <BOS>ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒìœ¼ë¡œ í•©ë‹ˆë‹¤. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ í•œêº¼ë²ˆì— ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë²¡í„°ë¡œ encodeí•´ ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]ë¥¼ í†µí•´ <BOS>ë¥¼ ì œì™¸\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ í•œêº¼ë²ˆì— ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
