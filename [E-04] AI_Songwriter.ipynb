{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5f987e",
   "metadata": {},
   "source": [
    "## 작사가 인공지능 만들기 🎼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11a041",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d3147",
   "metadata": {},
   "source": [
    "💬 인공지능이 인간의 언어를 이해할 수 있을까요? 더 나아가 스스로 글을 쓸 수도 있을까요? 오늘의 프로젝트는 **수많은 노래 가사들을 학습하고 스스로 가사를 쓰는 작사가 인공지능을 만드는 것입니다!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf4205",
   "metadata": {},
   "source": [
    "인간이 사용하는 언어를 이해하고 활용하는 머신러닝 분야를 **자연어처리(NLP:Natural Language Processing)** 라고 합니다. 음성인식 기술이나 파파고의 번역기술, 작문 AI GPT-3 등도 이 분야에 속합니다. 오늘은 NLP에서 자주 쓰이는 RNN 네트워크를 이용하여 모델을 구성할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e213a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83238d",
   "metadata": {},
   "source": [
    "### 0. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757cadc",
   "metadata": {},
   "source": [
    "순환신경망(RNN)은 이전에 입력받은 토큰을 기준으로 (통계적으로) 다음 토큰을 생성합니다. RNN 네트워크를 학습시키기 위해 **새로은 문장을 생성하는 토큰인`<start>`와 문장이 끝났음을 알리는 `<end>` 토큰을 이용해서 데이터를 생성한 후, 소스 데이터와 타겟 데이터로 분리할 것입니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8192bcf",
   "metadata": {},
   "source": [
    "훌륭한 작사가가 되려면 이미 쓰인 좋은 가사들을 많이 알면 도움이 되겠죠?\n",
    "아델부터 리한나까지 유명 가수들의 노래가사들을 모은 데이터를 활용해 모델을 학습시킬 것입니다. \n",
    "\n",
    "먼저 가사데이터를 불러오고 `raw_corpus`라는 리스트에 한 줄씩 담아줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa4a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd72db3",
   "metadata": {},
   "source": [
    "첫 세 줄을 불러와봤습니다. 구글링해보니 캐나다 출신의 가수 Leonard Cohen의 Hallelujah라는 곡이네요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffaa11",
   "metadata": {},
   "source": [
    "* **토큰화(Tokenize)**\n",
    "\n",
    "**토큰화는 자연어 문장을 모델이 처리하기 쉬운 크기로 끊어 단위화하는 것을 의미합니다.** 영어로 생각하면 단어 단위로, 한국어로 생각하면 어절을 단위로 토큰화를 해야겠네요. (더 정교하게 모델링하려면 형태소 단위로 끊어야 하려나요)\n",
    "\n",
    "여기서 다루는 데이터는 영어 가사이기 때문에 띄어쓰기를 단위로 끊어주면 됩니다. 다만 대문자 소문자의 구분이나 불필요한 문장 부호 등은 없애는 게 좋겠네요. 그리고 후에 소스와 타켓데이터로 분리하기 위해 문장 앞 뒤에 `<start>`와 `<end>`를 붙여줘야합니다. **원하는 방식으로 문장을 정제해 줄 함수 `preprocess_sentence`를 정의합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8729b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5657bf",
   "metadata": {},
   "source": [
    "`preprocess_sentence` 함수를 이용하여 원하는 형식으로 정제한 문장을 `corpus` 리스트에 담아줍니다. 너무 긴 문장은 다른 데이터들이 과도한 padding을 갖게 하고, 작사가 모델을 학습시키기에 알맞지 않을 수 있습니다. **따라서 토큰의 개수가 15개를 넘기는 경우 데이터에서 제외시키겠습니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042ebf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    # <start>와 <end> 토큰을 제외하고 토큰의 개수가 15개가 넘는 경우 제외합니다\n",
    "    if len(preprocessed_sentence.split()) > 17: continue\n",
    "    \n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16beb27f",
   "metadata": {},
   "source": [
    "문장이 잘 정제되어 담겨있음을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d310c63",
   "metadata": {},
   "source": [
    "문장을 띄어쓰기 단위로 끊어 읽을 수 있으니 토큰화가 다 된 것 같지만, 컴퓨터의 입장에서는 아닙니다. 컴퓨터는 언어를 이해하는 것이 아니라 언어를 숫자로 변형한 데이터를 이해하니까요. 따라서 **우리가 만든 문장들을 숫자로 변환해줘야 합니다. 이 과정을 벡터화(vectorize)라고 하고, 숫자로 변환된 데이터를 텐서(tensor)라고 합니다.**\n",
    "\n",
    "`corpus`에 담아둔 가사들을 텐서플로우의 `Tokenizer`와 `pad_sequences`를 사용하여 컴퓨터가 알아들을 수 있게 토큰화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18116967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2930 ...    0    0    0]\n",
      " [   2   32    7 ...    0    0    0]\n",
      " ...\n",
      " [   2  261  192 ...    0    0    0]\n",
      " [   2  132    4 ...   10 1070    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f4659e0ed30>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 15000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 15000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e40f7",
   "metadata": {},
   "source": [
    "우리가 가지고 있던 가사들이 숫자 데이터(tensor)로 바뀌어서 저장된 것을 볼 수 있습니다.\n",
    "\n",
    "그럼 각 숫자 인덱스에 배당된 단어를 살펴볼까요. `tokenizer`라는 곳에 단어사전이 생성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5b426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7c545",
   "metadata": {},
   "source": [
    "이제 텐서들을 `<start>`로 시작하는 소스 데이터와 `<end>`로 끝나는 타겟 데이터로 분리해 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3265a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  308   62   55    9  972 6004    3    0    0    0\n",
      "    0    0]\n",
      "[  50    4   95  308   62   55    9  972 6004    3    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe63c26",
   "metadata": {},
   "source": [
    "여태까지 토큰화한 데이터를 `tensorflow`에서 활용될 데이터셋 형태로 만들어 줍니다. `tf.data.Dataset.from_tensor_slices()`를 이용하여 `tf.data.Dataset` 객체를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461868f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 16), (256, 16)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 15000개와, 여기 포함되지 않은 0:<pad>를 포함하여 15001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba1863",
   "metadata": {},
   "source": [
    "이제 훈련 데이터와 평가 데이터를 분리합니다. `sklearn`의 `train_test_split()`을 이용합니다. 학습 데이터와 평가 데이터의 비율은 8:2 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9dff965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train 개수:  139852 , enc_val 개수:  34964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=2)\n",
    "                                                        \n",
    "\n",
    "print('enc_train 개수: ', len(enc_train),', enc_val 개수: ', len(enc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bdc8560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((139852, 16), (139852, 16))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape, dec_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2aaf9e",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e12bf5",
   "metadata": {},
   "source": [
    "### 1. 모델 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b9f45",
   "metadata": {},
   "source": [
    "1개의 `Embedding` 레이어, 2개의 `LSTM` 레이어, 1개의 `Dense` 레이어로 구성된 모델을 생성합니다. \n",
    "\n",
    "`Embedding` 레이어는 텐서의 숫자를 워드 벡터로 바꿔줍니다. 워드 벡터란 해당 단어의 의미를 해부한 벡터입니다. 즉 `Embedding` 레이어를 통해서 단어의 의미와 단어들 사이의 연관성을 표현합니다.\n",
    "\n",
    "`LSTM`은 `Long Short-Term Memory`의 약자로(세상 이런 모순적인 말이 있나?) RNN 네트워크의 한 종류입니다.. RNN의 특징은 시퀀스한 데이터에 기반하여 타겟 데이터를 예측하는데, 필요한 정보까지의 거리가 먼 경우 어려움이 생깁니다. 이런 문제를 \"긴 기간의 의존성(long-term dependencies)\"라고 하는데, `LSTM`은 이를 해결하기 위해 만들어진 새로운 RNN 네트워크입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145ab489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f59bd",
   "metadata": {},
   "source": [
    "`embedding_size`: 워드 벡터의 차원 수(단어가 추상적으로 표현되는 정도)\n",
    "\n",
    "`hidden_size`: LSTM 레이어의 hidden state 의 차원수(≒일하는 일꾼의 수!)\n",
    "\n",
    "👉 loss의 최소화를 위해서 수치 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da6d740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 16, 15001), dtype=float32, numpy=\n",
       "array([[[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [ 5.9822265e-05, -1.0874363e-04,  5.5842876e-05, ...,\n",
       "          2.3249318e-05,  6.4104836e-04,  3.9525999e-04],\n",
       "        [ 2.3417514e-04, -4.2893909e-05, -1.6348301e-04, ...,\n",
       "          2.7113850e-04,  6.5880781e-04,  3.5857587e-04],\n",
       "        ...,\n",
       "        [ 2.6840535e-03,  2.0165492e-03,  1.1301689e-03, ...,\n",
       "          3.0082683e-03, -2.3508999e-03, -3.0586624e-03],\n",
       "        [ 3.0192370e-03,  2.2273872e-03,  1.3146029e-03, ...,\n",
       "          3.1658052e-03, -2.5453919e-03, -3.3218863e-03],\n",
       "        [ 3.3144697e-03,  2.4131318e-03,  1.4891539e-03, ...,\n",
       "          3.2713385e-03, -2.7116395e-03, -3.5514084e-03]],\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [ 2.7969852e-05, -4.3367987e-04,  4.6673900e-04, ...,\n",
       "          1.3746547e-04,  4.4456581e-04,  1.5222898e-04],\n",
       "        [ 5.5569650e-05, -4.0336026e-04,  6.3152058e-04, ...,\n",
       "          4.9523271e-05,  7.6376641e-04,  1.5401065e-04],\n",
       "        ...,\n",
       "        [ 2.4361555e-03,  2.2325011e-03,  1.5777940e-03, ...,\n",
       "          2.8885081e-03, -2.1948686e-03, -2.6050864e-03],\n",
       "        [ 2.7818552e-03,  2.4079885e-03,  1.7083336e-03, ...,\n",
       "          3.0394255e-03, -2.4209521e-03, -2.9228197e-03],\n",
       "        [ 3.0945502e-03,  2.5595855e-03,  1.8308047e-03, ...,\n",
       "          3.1416016e-03, -2.6131475e-03, -3.2034440e-03]],\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [-7.4934149e-05, -1.9410309e-04,  3.3760161e-04, ...,\n",
       "          2.8940834e-04,  6.3549133e-04,  3.4835638e-04],\n",
       "        [-5.9862286e-05, -2.8167511e-05,  4.2180705e-04, ...,\n",
       "          3.1486960e-04,  8.7655522e-04,  3.4593639e-04],\n",
       "        ...,\n",
       "        [ 1.2741915e-03,  1.0126878e-03,  2.6442486e-04, ...,\n",
       "          1.5408130e-03, -1.2859423e-03,  3.4110932e-04],\n",
       "        [ 1.6274486e-03,  1.2966886e-03,  4.4709895e-04, ...,\n",
       "          2.0543213e-03, -1.5844860e-03, -1.6921994e-04],\n",
       "        [ 1.9817827e-03,  1.5853909e-03,  6.4384611e-04, ...,\n",
       "          2.4867791e-03, -1.8580804e-03, -6.8115472e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [-1.2422906e-06, -2.7593182e-04,  2.0604079e-04, ...,\n",
       "          1.1054342e-05,  3.1851229e-04,  3.1637144e-04],\n",
       "        [ 1.8312108e-04, -2.0797041e-04,  2.7158760e-04, ...,\n",
       "          3.4481598e-04,  1.4525371e-04,  1.6072295e-04],\n",
       "        ...,\n",
       "        [ 3.7326482e-03,  2.6499396e-03,  1.8302499e-03, ...,\n",
       "          3.2812308e-03, -2.9089907e-03, -3.5802394e-03],\n",
       "        [ 3.9221989e-03,  2.7821348e-03,  1.9509103e-03, ...,\n",
       "          3.2857687e-03, -3.0275264e-03, -3.7735272e-03],\n",
       "        [ 4.0872218e-03,  2.8947210e-03,  2.0581526e-03, ...,\n",
       "          3.2818019e-03, -3.1239842e-03, -3.9445665e-03]],\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [ 2.0468935e-04, -1.3632749e-04,  1.2412926e-04, ...,\n",
       "          2.9125661e-04,  4.0665464e-04,  3.5920594e-04],\n",
       "        [ 3.5937884e-04, -3.7587790e-05, -7.9579862e-05, ...,\n",
       "          4.1565619e-04,  5.5027829e-04,  5.7326013e-04],\n",
       "        ...,\n",
       "        [-1.0678382e-03,  1.1537182e-03,  1.3262947e-04, ...,\n",
       "          2.7555507e-05,  1.7218051e-03,  1.9297749e-04],\n",
       "        [-8.1744936e-04,  9.2641939e-04,  1.8926943e-04, ...,\n",
       "         -2.1675529e-04,  1.5444688e-03,  8.8924106e-05],\n",
       "        [-7.2459551e-04,  8.2538137e-04,  1.2267513e-04, ...,\n",
       "         -2.2333322e-04,  1.5302909e-03, -1.2008182e-04]],\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [-1.2422906e-06, -2.7593182e-04,  2.0604079e-04, ...,\n",
       "          1.1054342e-05,  3.1851229e-04,  3.1637144e-04],\n",
       "        [ 1.8312108e-04, -2.0797041e-04,  2.7158760e-04, ...,\n",
       "          3.4481598e-04,  1.4525371e-04,  1.6072295e-04],\n",
       "        ...,\n",
       "        [ 3.7326482e-03,  2.6499396e-03,  1.8302499e-03, ...,\n",
       "          3.2812308e-03, -2.9089907e-03, -3.5802394e-03],\n",
       "        [ 3.9221989e-03,  2.7821348e-03,  1.9509103e-03, ...,\n",
       "          3.2857687e-03, -3.0275264e-03, -3.7735272e-03],\n",
       "        [ 4.0872218e-03,  2.8947210e-03,  2.0581526e-03, ...,\n",
       "          3.2818019e-03, -3.1239842e-03, -3.9445665e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dca1991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 32,855,961\n",
      "Trainable params: 32,855,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a20a2",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2d687",
   "metadata": {},
   "source": [
    "### 2. 모델 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0f57a",
   "metadata": {},
   "source": [
    "모델을 학습시키고 `loss`값이 2.2 수준이 될 때까지 `embedding_size`와 `hidden_size`를 조절합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca80d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "682/682 [==============================] - 135s 194ms/step - loss: 2.9956\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 140s 205ms/step - loss: 2.5568\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 140s 206ms/step - loss: 2.4081\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 2.2949\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 2.1971\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 2.1080\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 2.0263\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 1.9510\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 140s 206ms/step - loss: 1.8815\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 140s 206ms/step - loss: 1.8162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4524f962b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854f1c1",
   "metadata": {},
   "source": [
    "😏 휴우~ 시간이 굉장히 오래걸리네요! \n",
    "\n",
    "변수를 따로 조절하지 않았는데도 loss 값이 2.2보다 적게 나왔습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8159c0a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba5bad",
   "metadata": {},
   "source": [
    "### 3. 잘 만들어졌는지 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899b9f5",
   "metadata": {},
   "source": [
    "작사가 모델을 이용해 노랫말을 생성해 봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786f8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"I\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "131df78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a beautiful thing <end> '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a629a9",
   "metadata": {},
   "source": [
    "오~ 그럴 듯한 가사를 써 내는군요 🤩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e86748dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hey , hey , hey , hey , hey <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> hey\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71a16faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> baby , baby , baby , baby , baby , baby , baby <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> baby\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b76c0",
   "metadata": {},
   "source": [
    "'Hallelujah'의 예처럼, 반복되는 가사가 학습데이터로 많이 들어가서 그런지 같은 단어가 반복되는 가사를 많이 만들어 내네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ab7c00dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> come on motherfuckers come on <end> '"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> come\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a5a8980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what you want nixga what you what you want nixga <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555ecf3",
   "metadata": {},
   "source": [
    "힙합 가사의 영향인지 욕도 많이.. 나오네요.. 😇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982c46c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> let s go <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> let\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "543da434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> why you wanna see me in the dark <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> why\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "025d7611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m the one <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd2eb0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the only one who knows <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "099041af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s got me runnin round and round <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea48be1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a sportsman , a gypsy , a gypsy <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb66163",
   "metadata": {},
   "source": [
    "다양한 단어를 시작 토큰으로 넣어줘도 꽤 괜찮은 가사를 만들어내는 것을 볼 수 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfe961",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631fc65",
   "metadata": {},
   "source": [
    "하이퍼파라미터의 변화에 따른 차이를 보기 위해서 `model2`도 만들어 봅니다.\n",
    "\n",
    "`embedding_size`와 `hidden_size`를 각각 두 배 씩 늘려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fce56fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "model2 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b72cf081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "682/682 [==============================] - 402s 562ms/step - loss: 2.9963\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 385s 565ms/step - loss: 2.5025\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 2.2813\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 2.0832\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.8971\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.7232\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.5579\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.4015\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.2577\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.1314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f455bfea490>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model2.compile(loss=loss, optimizer=optimizer)\n",
    "model2.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c2815",
   "metadata": {},
   "source": [
    "시간이 전보다도 더 걸렸네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c371c80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m gonna make you feel so good <end> '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> I\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "908d6ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m bad , i m bad you know it <end> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f23977d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s got me runnin round and round she tastes like the sunshine kissing me <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> she\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c74d34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s the only one for me <end> '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> he\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f491801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> baby , i got a plan <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> baby\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30233868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hey , hey , hey , hey , hey , hey <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> hey\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c370faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> tell me what i wanna hear <end> '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> tell me\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5140993",
   "metadata": {},
   "source": [
    "여전히 가사를 잘 뽑아냅니다. 반복되는 가사는 이전 모델보다 조금 줄었네요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c70f9",
   "metadata": {},
   "source": [
    "`model3`도 만들어 봅니다.\n",
    "\n",
    "`embedding_size`와 `hidden_size`를 각각 두 배 씩 줄여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d1b3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 512\n",
    "model3 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88a8112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "682/682 [==============================] - 63s 88ms/step - loss: 3.2883\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 61s 89ms/step - loss: 2.7303\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 61s 89ms/step - loss: 2.5833\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.4891\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.4107\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.3384\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.2699\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.2043\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.1410\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.0808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4558ae2400>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model3.compile(loss=loss, optimizer=optimizer)\n",
    "model3.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c98b8425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m the one that s gon be alright <end> '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> I\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50382a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the only one that s in the zone <end> '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "140c544b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s a <unk> , <end> '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> she\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0b32986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a <unk> , <end> '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> he\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "340105f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> baby , baby , baby , baby , baby <end> '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> baby\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eccd558e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hey , hey , hey , hey , hey , hey , hey , hey , hey , hey '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> hey\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd980249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> tell me what you want <end> '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> tell me\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2ebc76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what you want nixga what you want nixga what you want nixga <end> '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> what\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57876fb",
   "metadata": {},
   "source": [
    "전체적으로 `model3`는 첫번째 `model`과 비슷한 느낌이네요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560fe9c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d46cb1",
   "metadata": {},
   "source": [
    "### 🤔 회고\n",
    "\n",
    "#### 1. 결과 이미지를 바로바로 확인하면서 진행했던 CV 프로젝트와는 다르게 데이터 전처리하는 과정이 굉장히 길고 복잡하게 느껴져서 초반부에는 시간이 오래걸렸습니다.\n",
    "\n",
    "#### 2. 그런데 막상 모델을 만들고 직접 가사를 생성해보니 이렇게 재미있을 수가 있을까요? NLP... 넘나 매력적인 분야인 것 같습니다.\n",
    "\n",
    "#### 3. 익플 프로젝트는 말그대로 '탐험'이기 때문에 모델을 깊게 연구하지는 않고 바로바로 활용하지만 단어 사이의 의미 벡터를 만들고 한 단어 다음에 나올 단어를 통계적으로 추출해낸다는 것이 놀라운 기술이었습니다. \n",
    "\n",
    "#### 4. **`Embedding layer`와 `RNN layer`가 어떤 원리로 작용하는지 좀 더 깊이 알아봐야겠다는 생각이 들었습니다.**\n",
    "\n",
    "#### 5. 아 그런데, `train` 데이터와 `test` 데이터는 왜 굳이 나뉜거죠..? `train` 데이터로만 모델을 만들고 `test`로 실제 예측값과 비교를 해봤어야 하는 건가요..? 😵\n",
    "\n",
    "#### 6. 하이퍼파라미터 값을 바꾸면서 모델을 만들어봤는데 개인적으로는 첫번째 `model2`가 가장 자연스럽게 느껴졌습니다. 수치적인 결과값이 나오는게 아니어서 모델의 설계자에 따라서 최종 모델이 다르게 설정될 것 같네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f91e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
