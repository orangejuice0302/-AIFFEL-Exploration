{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5f987e",
   "metadata": {},
   "source": [
    "## ì‘ì‚¬ê°€ ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸° ğŸ¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11a041",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d3147",
   "metadata": {},
   "source": [
    "ğŸ’¬ ì¸ê³µì§€ëŠ¥ì´ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•  ìˆ˜ ìˆì„ê¹Œìš”? ë” ë‚˜ì•„ê°€ ìŠ¤ìŠ¤ë¡œ ê¸€ì„ ì“¸ ìˆ˜ë„ ìˆì„ê¹Œìš”? ì˜¤ëŠ˜ì˜ í”„ë¡œì íŠ¸ëŠ” **ìˆ˜ë§ì€ ë…¸ë˜ ê°€ì‚¬ë“¤ì„ í•™ìŠµí•˜ê³  ìŠ¤ìŠ¤ë¡œ ê°€ì‚¬ë¥¼ ì“°ëŠ” ì‘ì‚¬ê°€ ì¸ê³µì§€ëŠ¥ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf4205",
   "metadata": {},
   "source": [
    "ì¸ê°„ì´ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  í™œìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì•¼ë¥¼ **ìì—°ì–´ì²˜ë¦¬(NLP:Natural Language Processing)** ë¼ê³  í•©ë‹ˆë‹¤. ìŒì„±ì¸ì‹ ê¸°ìˆ ì´ë‚˜ íŒŒíŒŒê³ ì˜ ë²ˆì—­ê¸°ìˆ , ì‘ë¬¸ AI GPT-3 ë“±ë„ ì´ ë¶„ì•¼ì— ì†í•©ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ NLPì—ì„œ ìì£¼ ì“°ì´ëŠ” RNN ë„¤íŠ¸ì›Œí¬ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e213a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83238d",
   "metadata": {},
   "source": [
    "### 0. ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757cadc",
   "metadata": {},
   "source": [
    "ìˆœí™˜ì‹ ê²½ë§(RNN)ì€ ì´ì „ì— ì…ë ¥ë°›ì€ í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ (í†µê³„ì ìœ¼ë¡œ) ë‹¤ìŒ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤. RNN ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ **ìƒˆë¡œì€ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” í† í°ì¸`<start>`ì™€ ë¬¸ì¥ì´ ëë‚¬ìŒì„ ì•Œë¦¬ëŠ” `<end>` í† í°ì„ ì´ìš©í•´ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•œ í›„, ì†ŒìŠ¤ ë°ì´í„°ì™€ íƒ€ê²Ÿ ë°ì´í„°ë¡œ ë¶„ë¦¬í•  ê²ƒì…ë‹ˆë‹¤.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8192bcf",
   "metadata": {},
   "source": [
    "í›Œë¥­í•œ ì‘ì‚¬ê°€ê°€ ë˜ë ¤ë©´ ì´ë¯¸ ì“°ì¸ ì¢‹ì€ ê°€ì‚¬ë“¤ì„ ë§ì´ ì•Œë©´ ë„ì›€ì´ ë˜ê² ì£ ?\n",
    "ì•„ë¸ë¶€í„° ë¦¬í•œë‚˜ê¹Œì§€ ìœ ëª… ê°€ìˆ˜ë“¤ì˜ ë…¸ë˜ê°€ì‚¬ë“¤ì„ ëª¨ì€ ë°ì´í„°ë¥¼ í™œìš©í•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ê²ƒì…ë‹ˆë‹¤. \n",
    "\n",
    "ë¨¼ì € ê°€ì‚¬ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  `raw_corpus`ë¼ëŠ” ë¦¬ìŠ¤íŠ¸ì— í•œ ì¤„ì”© ë‹´ì•„ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa4a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpus ì— ë‹´ìŠµë‹ˆë‹¤.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd72db3",
   "metadata": {},
   "source": [
    "ì²« ì„¸ ì¤„ì„ ë¶ˆëŸ¬ì™€ë´¤ìŠµë‹ˆë‹¤. êµ¬ê¸€ë§í•´ë³´ë‹ˆ ìºë‚˜ë‹¤ ì¶œì‹ ì˜ ê°€ìˆ˜ Leonard Cohenì˜ Hallelujahë¼ëŠ” ê³¡ì´ë„¤ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffaa11",
   "metadata": {},
   "source": [
    "* **í† í°í™”(Tokenize)**\n",
    "\n",
    "**í† í°í™”ëŠ” ìì—°ì–´ ë¬¸ì¥ì„ ëª¨ë¸ì´ ì²˜ë¦¬í•˜ê¸° ì‰¬ìš´ í¬ê¸°ë¡œ ëŠì–´ ë‹¨ìœ„í™”í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.** ì˜ì–´ë¡œ ìƒê°í•˜ë©´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ, í•œêµ­ì–´ë¡œ ìƒê°í•˜ë©´ ì–´ì ˆì„ ë‹¨ìœ„ë¡œ í† í°í™”ë¥¼ í•´ì•¼ê² ë„¤ìš”. (ë” ì •êµí•˜ê²Œ ëª¨ë¸ë§í•˜ë ¤ë©´ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ëŠì–´ì•¼ í•˜ë ¤ë‚˜ìš”)\n",
    "\n",
    "ì—¬ê¸°ì„œ ë‹¤ë£¨ëŠ” ë°ì´í„°ëŠ” ì˜ì–´ ê°€ì‚¬ì´ê¸° ë•Œë¬¸ì— ë„ì–´ì“°ê¸°ë¥¼ ë‹¨ìœ„ë¡œ ëŠì–´ì£¼ë©´ ë©ë‹ˆë‹¤. ë‹¤ë§Œ ëŒ€ë¬¸ì ì†Œë¬¸ìì˜ êµ¬ë¶„ì´ë‚˜ ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ë¶€í˜¸ ë“±ì€ ì—†ì• ëŠ” ê²Œ ì¢‹ê² ë„¤ìš”. ê·¸ë¦¬ê³  í›„ì— ì†ŒìŠ¤ì™€ íƒ€ì¼“ë°ì´í„°ë¡œ ë¶„ë¦¬í•˜ê¸° ìœ„í•´ ë¬¸ì¥ ì• ë’¤ì— `<start>`ì™€ `<end>`ë¥¼ ë¶™ì—¬ì¤˜ì•¼í•©ë‹ˆë‹¤. **ì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¬¸ì¥ì„ ì •ì œí•´ ì¤„ í•¨ìˆ˜ `preprocess_sentence`ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8729b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# ì…ë ¥ëœ ë¬¸ì¥ì„\n",
    "#     1. ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³ , ì–‘ìª½ ê³µë°±ì„ ì§€ì›ë‹ˆë‹¤\n",
    "#     2. íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ë„£ê³ \n",
    "#     3. ì—¬ëŸ¬ê°œì˜ ê³µë°±ì€ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤\n",
    "#     4. a-zA-Z?.!,Â¿ê°€ ì•„ë‹Œ ëª¨ë“  ë¬¸ìë¥¼ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤\n",
    "#     5. ë‹¤ì‹œ ì–‘ìª½ ê³µë°±ì„ ì§€ì›ë‹ˆë‹¤\n",
    "#     6. ë¬¸ì¥ ì‹œì‘ì—ëŠ” <start>, ëì—ëŠ” <end>ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤\n",
    "# ì´ ìˆœì„œë¡œ ì²˜ë¦¬í•´ì£¼ë©´ ë¬¸ì œê°€ ë˜ëŠ” ìƒí™©ì„ ë°©ì§€í•  ìˆ˜ ìˆê² ë„¤ìš”!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5657bf",
   "metadata": {},
   "source": [
    "`preprocess_sentence` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì •ì œí•œ ë¬¸ì¥ì„ `corpus` ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì¤ë‹ˆë‹¤. ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ë‹¤ë¥¸ ë°ì´í„°ë“¤ì´ ê³¼ë„í•œ paddingì„ ê°–ê²Œ í•˜ê³ , ì‘ì‚¬ê°€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸°ì— ì•Œë§ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ë”°ë¼ì„œ í† í°ì˜ ê°œìˆ˜ê°€ 15ê°œë¥¼ ë„˜ê¸°ëŠ” ê²½ìš° ë°ì´í„°ì—ì„œ ì œì™¸ì‹œí‚¤ê² ìŠµë‹ˆë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042ebf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    \n",
    "    # ì •ì œë¥¼ í•˜ê³  ë‹´ì•„ì£¼ì„¸ìš”\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    # <start>ì™€ <end> í† í°ì„ ì œì™¸í•˜ê³  í† í°ì˜ ê°œìˆ˜ê°€ 15ê°œê°€ ë„˜ëŠ” ê²½ìš° ì œì™¸í•©ë‹ˆë‹¤\n",
    "    if len(preprocessed_sentence.split()) > 17: continue\n",
    "    \n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# ì •ì œëœ ê²°ê³¼ë¥¼ 10ê°œë§Œ í™•ì¸í•´ë³´ì£ \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16beb27f",
   "metadata": {},
   "source": [
    "ë¬¸ì¥ì´ ì˜ ì •ì œë˜ì–´ ë‹´ê²¨ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d310c63",
   "metadata": {},
   "source": [
    "ë¬¸ì¥ì„ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ëŠì–´ ì½ì„ ìˆ˜ ìˆìœ¼ë‹ˆ í† í°í™”ê°€ ë‹¤ ëœ ê²ƒ ê°™ì§€ë§Œ, ì»´í“¨í„°ì˜ ì…ì¥ì—ì„œëŠ” ì•„ë‹™ë‹ˆë‹¤. ì»´í“¨í„°ëŠ” ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì–¸ì–´ë¥¼ ìˆ«ìë¡œ ë³€í˜•í•œ ë°ì´í„°ë¥¼ ì´í•´í•˜ë‹ˆê¹Œìš”. ë”°ë¼ì„œ **ìš°ë¦¬ê°€ ë§Œë“  ë¬¸ì¥ë“¤ì„ ìˆ«ìë¡œ ë³€í™˜í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ ë²¡í„°í™”(vectorize)ë¼ê³  í•˜ê³ , ìˆ«ìë¡œ ë³€í™˜ëœ ë°ì´í„°ë¥¼ í…ì„œ(tensor)ë¼ê³  í•©ë‹ˆë‹¤.**\n",
    "\n",
    "`corpus`ì— ë‹´ì•„ë‘” ê°€ì‚¬ë“¤ì„ í…ì„œí”Œë¡œìš°ì˜ `Tokenizer`ì™€ `pad_sequences`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»´í“¨í„°ê°€ ì•Œì•„ë“¤ì„ ìˆ˜ ìˆê²Œ í† í°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18116967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2930 ...    0    0    0]\n",
      " [   2   32    7 ...    0    0    0]\n",
      " ...\n",
      " [   2  261  192 ...    0    0    0]\n",
      " [   2  132    4 ...   10 1070    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f4659e0ed30>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 15000ë‹¨ì–´ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” tokenizerë¥¼ ë§Œë“¤ê²ë‹ˆë‹¤\n",
    "    # ìš°ë¦¬ëŠ” ì´ë¯¸ ë¬¸ì¥ì„ ì •ì œí–ˆìœ¼ë‹ˆ filtersê°€ í•„ìš”ì—†ì–´ìš”\n",
    "    # 15000ë‹¨ì–´ì— í¬í•¨ë˜ì§€ ëª»í•œ ë‹¨ì–´ëŠ” '<unk>'ë¡œ ë°”ê¿€ê±°ì—ìš”\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpusë¥¼ ì´ìš©í•´ tokenizer ë‚´ë¶€ì˜ ë‹¨ì–´ì¥ì„ ì™„ì„±í•©ë‹ˆë‹¤\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # ì¤€ë¹„í•œ tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤ë‹ˆë‹¤\n",
    "    # ë§Œì•½ ì‹œí€€ìŠ¤ê°€ ì§§ë‹¤ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.\n",
    "    # ë¬¸ì¥ ì•ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶”ê³  ì‹¶ë‹¤ë©´ padding='pre'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e40f7",
   "metadata": {},
   "source": [
    "ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆë˜ ê°€ì‚¬ë“¤ì´ ìˆ«ì ë°ì´í„°(tensor)ë¡œ ë°”ë€Œì–´ì„œ ì €ì¥ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ¼ ê° ìˆ«ì ì¸ë±ìŠ¤ì— ë°°ë‹¹ëœ ë‹¨ì–´ë¥¼ ì‚´í´ë³¼ê¹Œìš”. `tokenizer`ë¼ëŠ” ê³³ì— ë‹¨ì–´ì‚¬ì „ì´ ìƒì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5b426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7c545",
   "metadata": {},
   "source": [
    "ì´ì œ í…ì„œë“¤ì„ `<start>`ë¡œ ì‹œì‘í•˜ëŠ” ì†ŒìŠ¤ ë°ì´í„°ì™€ `<end>`ë¡œ ëë‚˜ëŠ” íƒ€ê²Ÿ ë°ì´í„°ë¡œ ë¶„ë¦¬í•´ ì¤ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3265a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  308   62   55    9  972 6004    3    0    0    0\n",
      "    0    0]\n",
      "[  50    4   95  308   62   55    9  972 6004    3    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "# ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe63c26",
   "metadata": {},
   "source": [
    "ì—¬íƒœê¹Œì§€ í† í°í™”í•œ ë°ì´í„°ë¥¼ `tensorflow`ì—ì„œ í™œìš©ë  ë°ì´í„°ì…‹ í˜•íƒœë¡œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤. `tf.data.Dataset.from_tensor_slices()`ë¥¼ ì´ìš©í•˜ì—¬ `tf.data.Dataset` ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461868f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 16), (256, 16)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 15000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨í•˜ì—¬ 15001ê°œ\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# ì¤€ë¹„í•œ ë°ì´í„° ì†ŒìŠ¤ë¡œë¶€í„° ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba1863",
   "metadata": {},
   "source": [
    "ì´ì œ í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤. `sklearn`ì˜ `train_test_split()`ì„ ì´ìš©í•©ë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ì˜ ë¹„ìœ¨ì€ 8:2 ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9dff965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train ê°œìˆ˜:  139852 , enc_val ê°œìˆ˜:  34964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=2)\n",
    "                                                        \n",
    "\n",
    "print('enc_train ê°œìˆ˜: ', len(enc_train),', enc_val ê°œìˆ˜: ', len(enc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bdc8560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((139852, 16), (139852, 16))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape, dec_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2aaf9e",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e12bf5",
   "metadata": {},
   "source": [
    "### 1. ëª¨ë¸ ì„¤ê³„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b9f45",
   "metadata": {},
   "source": [
    "1ê°œì˜ `Embedding` ë ˆì´ì–´, 2ê°œì˜ `LSTM` ë ˆì´ì–´, 1ê°œì˜ `Dense` ë ˆì´ì–´ë¡œ êµ¬ì„±ëœ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "\n",
    "`Embedding` ë ˆì´ì–´ëŠ” í…ì„œì˜ ìˆ«ìë¥¼ ì›Œë“œ ë²¡í„°ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤. ì›Œë“œ ë²¡í„°ë€ í•´ë‹¹ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ í•´ë¶€í•œ ë²¡í„°ì…ë‹ˆë‹¤. ì¦‰ `Embedding` ë ˆì´ì–´ë¥¼ í†µí•´ì„œ ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì—°ê´€ì„±ì„ í‘œí˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "`LSTM`ì€ `Long Short-Term Memory`ì˜ ì•½ìë¡œ(ì„¸ìƒ ì´ëŸ° ëª¨ìˆœì ì¸ ë§ì´ ìˆë‚˜?) RNN ë„¤íŠ¸ì›Œí¬ì˜ í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤.. RNNì˜ íŠ¹ì§•ì€ ì‹œí€€ìŠ¤í•œ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë°, í•„ìš”í•œ ì •ë³´ê¹Œì§€ì˜ ê±°ë¦¬ê°€ ë¨¼ ê²½ìš° ì–´ë ¤ì›€ì´ ìƒê¹ë‹ˆë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ \"ê¸´ ê¸°ê°„ì˜ ì˜ì¡´ì„±(long-term dependencies)\"ë¼ê³  í•˜ëŠ”ë°, `LSTM`ì€ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ìƒˆë¡œìš´ RNN ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145ab489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f59bd",
   "metadata": {},
   "source": [
    "`embedding_size`: ì›Œë“œ ë²¡í„°ì˜ ì°¨ì› ìˆ˜(ë‹¨ì–´ê°€ ì¶”ìƒì ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ì •ë„)\n",
    "\n",
    "`hidden_size`: LSTM ë ˆì´ì–´ì˜ hidden state ì˜ ì°¨ì›ìˆ˜(â‰’ì¼í•˜ëŠ” ì¼ê¾¼ì˜ ìˆ˜!)\n",
    "\n",
    "ğŸ‘‰ lossì˜ ìµœì†Œí™”ë¥¼ ìœ„í•´ì„œ ìˆ˜ì¹˜ ì¡°ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da6d740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 16, 15001), dtype=float32, numpy=\n",
       "array([[[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [ 5.9822265e-05, -1.0874363e-04,  5.5842876e-05, ...,\n",
       "          2.3249318e-05,  6.4104836e-04,  3.9525999e-04],\n",
       "        [ 2.3417514e-04, -4.2893909e-05, -1.6348301e-04, ...,\n",
       "          2.7113850e-04,  6.5880781e-04,  3.5857587e-04],\n",
       "        ...,\n",
       "        [ 2.6840535e-03,  2.0165492e-03,  1.1301689e-03, ...,\n",
       "          3.0082683e-03, -2.3508999e-03, -3.0586624e-03],\n",
       "        [ 3.0192370e-03,  2.2273872e-03,  1.3146029e-03, ...,\n",
       "          3.1658052e-03, -2.5453919e-03, -3.3218863e-03],\n",
       "        [ 3.3144697e-03,  2.4131318e-03,  1.4891539e-03, ...,\n",
       "          3.2713385e-03, -2.7116395e-03, -3.5514084e-03]],\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [ 2.7969852e-05, -4.3367987e-04,  4.6673900e-04, ...,\n",
       "          1.3746547e-04,  4.4456581e-04,  1.5222898e-04],\n",
       "        [ 5.5569650e-05, -4.0336026e-04,  6.3152058e-04, ...,\n",
       "          4.9523271e-05,  7.6376641e-04,  1.5401065e-04],\n",
       "        ...,\n",
       "        [ 2.4361555e-03,  2.2325011e-03,  1.5777940e-03, ...,\n",
       "          2.8885081e-03, -2.1948686e-03, -2.6050864e-03],\n",
       "        [ 2.7818552e-03,  2.4079885e-03,  1.7083336e-03, ...,\n",
       "          3.0394255e-03, -2.4209521e-03, -2.9228197e-03],\n",
       "        [ 3.0945502e-03,  2.5595855e-03,  1.8308047e-03, ...,\n",
       "          3.1416016e-03, -2.6131475e-03, -3.2034440e-03]],\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [-7.4934149e-05, -1.9410309e-04,  3.3760161e-04, ...,\n",
       "          2.8940834e-04,  6.3549133e-04,  3.4835638e-04],\n",
       "        [-5.9862286e-05, -2.8167511e-05,  4.2180705e-04, ...,\n",
       "          3.1486960e-04,  8.7655522e-04,  3.4593639e-04],\n",
       "        ...,\n",
       "        [ 1.2741915e-03,  1.0126878e-03,  2.6442486e-04, ...,\n",
       "          1.5408130e-03, -1.2859423e-03,  3.4110932e-04],\n",
       "        [ 1.6274486e-03,  1.2966886e-03,  4.4709895e-04, ...,\n",
       "          2.0543213e-03, -1.5844860e-03, -1.6921994e-04],\n",
       "        [ 1.9817827e-03,  1.5853909e-03,  6.4384611e-04, ...,\n",
       "          2.4867791e-03, -1.8580804e-03, -6.8115472e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [-1.2422906e-06, -2.7593182e-04,  2.0604079e-04, ...,\n",
       "          1.1054342e-05,  3.1851229e-04,  3.1637144e-04],\n",
       "        [ 1.8312108e-04, -2.0797041e-04,  2.7158760e-04, ...,\n",
       "          3.4481598e-04,  1.4525371e-04,  1.6072295e-04],\n",
       "        ...,\n",
       "        [ 3.7326482e-03,  2.6499396e-03,  1.8302499e-03, ...,\n",
       "          3.2812308e-03, -2.9089907e-03, -3.5802394e-03],\n",
       "        [ 3.9221989e-03,  2.7821348e-03,  1.9509103e-03, ...,\n",
       "          3.2857687e-03, -3.0275264e-03, -3.7735272e-03],\n",
       "        [ 4.0872218e-03,  2.8947210e-03,  2.0581526e-03, ...,\n",
       "          3.2818019e-03, -3.1239842e-03, -3.9445665e-03]],\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [ 2.0468935e-04, -1.3632749e-04,  1.2412926e-04, ...,\n",
       "          2.9125661e-04,  4.0665464e-04,  3.5920594e-04],\n",
       "        [ 3.5937884e-04, -3.7587790e-05, -7.9579862e-05, ...,\n",
       "          4.1565619e-04,  5.5027829e-04,  5.7326013e-04],\n",
       "        ...,\n",
       "        [-1.0678382e-03,  1.1537182e-03,  1.3262947e-04, ...,\n",
       "          2.7555507e-05,  1.7218051e-03,  1.9297749e-04],\n",
       "        [-8.1744936e-04,  9.2641939e-04,  1.8926943e-04, ...,\n",
       "         -2.1675529e-04,  1.5444688e-03,  8.8924106e-05],\n",
       "        [-7.2459551e-04,  8.2538137e-04,  1.2267513e-04, ...,\n",
       "         -2.2333322e-04,  1.5302909e-03, -1.2008182e-04]],\n",
       "\n",
       "       [[ 2.1813596e-05, -1.3488010e-04,  1.1096840e-04, ...,\n",
       "          1.2949065e-04,  3.3164534e-04,  1.7096275e-04],\n",
       "        [-1.2422906e-06, -2.7593182e-04,  2.0604079e-04, ...,\n",
       "          1.1054342e-05,  3.1851229e-04,  3.1637144e-04],\n",
       "        [ 1.8312108e-04, -2.0797041e-04,  2.7158760e-04, ...,\n",
       "          3.4481598e-04,  1.4525371e-04,  1.6072295e-04],\n",
       "        ...,\n",
       "        [ 3.7326482e-03,  2.6499396e-03,  1.8302499e-03, ...,\n",
       "          3.2812308e-03, -2.9089907e-03, -3.5802394e-03],\n",
       "        [ 3.9221989e-03,  2.7821348e-03,  1.9509103e-03, ...,\n",
       "          3.2857687e-03, -3.0275264e-03, -3.7735272e-03],\n",
       "        [ 4.0872218e-03,  2.8947210e-03,  2.0581526e-03, ...,\n",
       "          3.2818019e-03, -3.1239842e-03, -3.9445665e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ì—ì„œ ë°ì´í„° í•œ ë°°ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# í•œ ë°°ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ì–´ë´…ë‹ˆë‹¤\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dca1991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 32,855,961\n",
      "Trainable params: 32,855,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a20a2",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2d687",
   "metadata": {},
   "source": [
    "### 2. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0f57a",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  `loss`ê°’ì´ 2.2 ìˆ˜ì¤€ì´ ë  ë•Œê¹Œì§€ `embedding_size`ì™€ `hidden_size`ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca80d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "682/682 [==============================] - 135s 194ms/step - loss: 2.9956\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 140s 205ms/step - loss: 2.5568\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 140s 206ms/step - loss: 2.4081\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 2.2949\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 2.1971\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 2.1080\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 2.0263\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 141s 206ms/step - loss: 1.9510\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 140s 206ms/step - loss: 1.8815\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 140s 206ms/step - loss: 1.8162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4524f962b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854f1c1",
   "metadata": {},
   "source": [
    "ğŸ˜ íœ´ìš°~ ì‹œê°„ì´ êµ‰ì¥íˆ ì˜¤ë˜ê±¸ë¦¬ë„¤ìš”! \n",
    "\n",
    "ë³€ìˆ˜ë¥¼ ë”°ë¡œ ì¡°ì ˆí•˜ì§€ ì•Šì•˜ëŠ”ë°ë„ loss ê°’ì´ 2.2ë³´ë‹¤ ì ê²Œ ë‚˜ì™”ìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8159c0a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba5bad",
   "metadata": {},
   "source": [
    "### 3. ì˜ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ í‰ê°€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899b9f5",
   "metadata": {},
   "source": [
    "ì‘ì‚¬ê°€ ëª¨ë¸ì„ ì´ìš©í•´ ë…¸ë«ë§ì„ ìƒì„±í•´ ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786f8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"I\", max_len=20):\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # ë‹¨ì–´ í•˜ë‚˜ì”© ì˜ˆì¸¡í•´ ë¬¸ì¥ì„ ë§Œë“­ë‹ˆë‹¤\n",
    "    #    1. ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤\n",
    "    #    2. ì˜ˆì¸¡ëœ ê°’ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì¸ word indexë¥¼ ë½‘ì•„ëƒ…ë‹ˆë‹¤\n",
    "    #    3. 2ì—ì„œ ì˜ˆì¸¡ëœ word indexë¥¼ ë¬¸ì¥ ë’¤ì— ë¶™ì…ë‹ˆë‹¤\n",
    "    #    4. ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í–ˆë‹¤ë©´ ë¬¸ì¥ ìƒì„±ì„ ë§ˆì¹©ë‹ˆë‹¤\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizerë¥¼ ì´ìš©í•´ word indexë¥¼ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤ \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "131df78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a beautiful thing <end> '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a629a9",
   "metadata": {},
   "source": [
    "ì˜¤~ ê·¸ëŸ´ ë“¯í•œ ê°€ì‚¬ë¥¼ ì¨ ë‚´ëŠ”êµ°ìš” ğŸ¤©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e86748dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hey , hey , hey , hey , hey <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> hey\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71a16faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> baby , baby , baby , baby , baby , baby , baby <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> baby\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b76c0",
   "metadata": {},
   "source": [
    "'Hallelujah'ì˜ ì˜ˆì²˜ëŸ¼, ë°˜ë³µë˜ëŠ” ê°€ì‚¬ê°€ í•™ìŠµë°ì´í„°ë¡œ ë§ì´ ë“¤ì–´ê°€ì„œ ê·¸ëŸ°ì§€ ê°™ì€ ë‹¨ì–´ê°€ ë°˜ë³µë˜ëŠ” ê°€ì‚¬ë¥¼ ë§ì´ ë§Œë“¤ì–´ ë‚´ë„¤ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ab7c00dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> come on motherfuckers come on <end> '"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> come\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a5a8980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what you want nixga what you what you want nixga <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555ecf3",
   "metadata": {},
   "source": [
    "í™í•© ê°€ì‚¬ì˜ ì˜í–¥ì¸ì§€ ìš•ë„ ë§ì´.. ë‚˜ì˜¤ë„¤ìš”.. ğŸ˜‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982c46c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> let s go <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> let\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "543da434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> why you wanna see me in the dark <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> why\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "025d7611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m the one <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd2eb0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the only one who knows <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "099041af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s got me runnin round and round <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea48be1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a sportsman , a gypsy , a gypsy <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb66163",
   "metadata": {},
   "source": [
    "ë‹¤ì–‘í•œ ë‹¨ì–´ë¥¼ ì‹œì‘ í† í°ìœ¼ë¡œ ë„£ì–´ì¤˜ë„ ê½¤ ê´œì°®ì€ ê°€ì‚¬ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfe961",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631fc65",
   "metadata": {},
   "source": [
    "í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ë³€í™”ì— ë”°ë¥¸ ì°¨ì´ë¥¼ ë³´ê¸° ìœ„í•´ì„œ `model2`ë„ ë§Œë“¤ì–´ ë´…ë‹ˆë‹¤.\n",
    "\n",
    "`embedding_size`ì™€ `hidden_size`ë¥¼ ê°ê° ë‘ ë°° ì”© ëŠ˜ë ¤ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fce56fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "model2 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b72cf081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "682/682 [==============================] - 402s 562ms/step - loss: 2.9963\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 385s 565ms/step - loss: 2.5025\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 2.2813\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 2.0832\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.8971\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.7232\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.5579\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.4015\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.2577\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 387s 567ms/step - loss: 1.1314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f455bfea490>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model2.compile(loss=loss, optimizer=optimizer)\n",
    "model2.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c2815",
   "metadata": {},
   "source": [
    "ì‹œê°„ì´ ì „ë³´ë‹¤ë„ ë” ê±¸ë ¸ë„¤ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c371c80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m gonna make you feel so good <end> '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> I\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "908d6ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m bad , i m bad you know it <end> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f23977d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s got me runnin round and round she tastes like the sunshine kissing me <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> she\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c74d34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s the only one for me <end> '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> he\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f491801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> baby , i got a plan <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> baby\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30233868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hey , hey , hey , hey , hey , hey <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> hey\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c370faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> tell me what i wanna hear <end> '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> tell me\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5140993",
   "metadata": {},
   "source": [
    "ì—¬ì „íˆ ê°€ì‚¬ë¥¼ ì˜ ë½‘ì•„ëƒ…ë‹ˆë‹¤. ë°˜ë³µë˜ëŠ” ê°€ì‚¬ëŠ” ì´ì „ ëª¨ë¸ë³´ë‹¤ ì¡°ê¸ˆ ì¤„ì—ˆë„¤ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c70f9",
   "metadata": {},
   "source": [
    "`model3`ë„ ë§Œë“¤ì–´ ë´…ë‹ˆë‹¤.\n",
    "\n",
    "`embedding_size`ì™€ `hidden_size`ë¥¼ ê°ê° ë‘ ë°° ì”© ì¤„ì—¬ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d1b3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 512\n",
    "model3 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88a8112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "682/682 [==============================] - 63s 88ms/step - loss: 3.2883\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 61s 89ms/step - loss: 2.7303\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 61s 89ms/step - loss: 2.5833\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.4891\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.4107\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.3384\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.2699\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.2043\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.1410\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 61s 90ms/step - loss: 2.0808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4558ae2400>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model3.compile(loss=loss, optimizer=optimizer)\n",
    "model3.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c98b8425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m the one that s gon be alright <end> '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> I\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50382a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the only one that s in the zone <end> '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "140c544b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s a <unk> , <end> '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> she\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0b32986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a <unk> , <end> '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> he\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "340105f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> baby , baby , baby , baby , baby <end> '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> baby\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eccd558e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hey , hey , hey , hey , hey , hey , hey , hey , hey , hey '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> hey\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd980249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> tell me what you want <end> '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> tell me\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2ebc76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what you want nixga what you want nixga what you want nixga <end> '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> what\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57876fb",
   "metadata": {},
   "source": [
    "ì „ì²´ì ìœ¼ë¡œ `model3`ëŠ” ì²«ë²ˆì§¸ `model`ê³¼ ë¹„ìŠ·í•œ ëŠë‚Œì´ë„¤ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560fe9c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d46cb1",
   "metadata": {},
   "source": [
    "### ğŸ¤” íšŒê³ \n",
    "\n",
    "#### 1. ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ë°”ë¡œë°”ë¡œ í™•ì¸í•˜ë©´ì„œ ì§„í–‰í–ˆë˜ CV í”„ë¡œì íŠ¸ì™€ëŠ” ë‹¤ë¥´ê²Œ ë°ì´í„° ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì´ êµ‰ì¥íˆ ê¸¸ê³  ë³µì¡í•˜ê²Œ ëŠê»´ì ¸ì„œ ì´ˆë°˜ë¶€ì—ëŠ” ì‹œê°„ì´ ì˜¤ë˜ê±¸ë ¸ìŠµë‹ˆë‹¤.\n",
    "\n",
    "#### 2. ê·¸ëŸ°ë° ë§‰ìƒ ëª¨ë¸ì„ ë§Œë“¤ê³  ì§ì ‘ ê°€ì‚¬ë¥¼ ìƒì„±í•´ë³´ë‹ˆ ì´ë ‡ê²Œ ì¬ë¯¸ìˆì„ ìˆ˜ê°€ ìˆì„ê¹Œìš”? NLP... ë„˜ë‚˜ ë§¤ë ¥ì ì¸ ë¶„ì•¼ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "#### 3. ìµí”Œ í”„ë¡œì íŠ¸ëŠ” ë§ê·¸ëŒ€ë¡œ 'íƒí—˜'ì´ê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ê¹Šê²Œ ì—°êµ¬í•˜ì§€ëŠ” ì•Šê³  ë°”ë¡œë°”ë¡œ í™œìš©í•˜ì§€ë§Œ ë‹¨ì–´ ì‚¬ì´ì˜ ì˜ë¯¸ ë²¡í„°ë¥¼ ë§Œë“¤ê³  í•œ ë‹¨ì–´ ë‹¤ìŒì— ë‚˜ì˜¬ ë‹¨ì–´ë¥¼ í†µê³„ì ìœ¼ë¡œ ì¶”ì¶œí•´ë‚¸ë‹¤ëŠ” ê²ƒì´ ë†€ë¼ìš´ ê¸°ìˆ ì´ì—ˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "#### 4. **`Embedding layer`ì™€ `RNN layer`ê°€ ì–´ë–¤ ì›ë¦¬ë¡œ ì‘ìš©í•˜ëŠ”ì§€ ì¢€ ë” ê¹Šì´ ì•Œì•„ë´ì•¼ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤.**\n",
    "\n",
    "#### 5. ì•„ ê·¸ëŸ°ë°, `train` ë°ì´í„°ì™€ `test` ë°ì´í„°ëŠ” ì™œ êµ³ì´ ë‚˜ë‰œê±°ì£ ..? `train` ë°ì´í„°ë¡œë§Œ ëª¨ë¸ì„ ë§Œë“¤ê³  `test`ë¡œ ì‹¤ì œ ì˜ˆì¸¡ê°’ê³¼ ë¹„êµë¥¼ í•´ë´¤ì–´ì•¼ í•˜ëŠ” ê±´ê°€ìš”..? ğŸ˜µ\n",
    "\n",
    "#### 6. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ë°”ê¾¸ë©´ì„œ ëª¨ë¸ì„ ë§Œë“¤ì–´ë´¤ëŠ”ë° ê°œì¸ì ìœ¼ë¡œëŠ” ì²«ë²ˆì§¸ `model2`ê°€ ê°€ì¥ ìì—°ìŠ¤ëŸ½ê²Œ ëŠê»´ì¡ŒìŠµë‹ˆë‹¤. ìˆ˜ì¹˜ì ì¸ ê²°ê³¼ê°’ì´ ë‚˜ì˜¤ëŠ”ê²Œ ì•„ë‹ˆì–´ì„œ ëª¨ë¸ì˜ ì„¤ê³„ìì— ë”°ë¼ì„œ ìµœì¢… ëª¨ë¸ì´ ë‹¤ë¥´ê²Œ ì„¤ì •ë  ê²ƒ ê°™ë„¤ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f91e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
